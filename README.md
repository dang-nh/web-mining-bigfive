# ğŸ§  Detection of Big Five Personality Traits from X Posts using ML

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Streamlit](https://img.shields.io/badge/Streamlit-1.28+-red.svg)](https://streamlit.io/)

Dá»± Ã¡n Web Mining capstone - phÃ¡t hiá»‡n tÃ­nh cÃ¡ch Big Five (OCEAN) tá»« cÃ¡c bÃ i Ä‘Äƒng máº¡ng xÃ£ há»™i, tÃ­ch há»£p:
- **Information Retrieval (IR)** - Truy xuáº¥t báº±ng chá»©ng vá»›i BM25
- **Opinion Mining** - PhÃ¢n tÃ­ch cáº£m xÃºc & emotion vá»›i CardiffNLP
- **Recommendation Systems** - Gá»£i Ã½ hashtag dá»±a trÃªn tÃ­nh cÃ¡ch
- **RAG (LLM)** - Giáº£i thÃ­ch dá»± Ä‘oÃ¡n vá»›i retrieval-augmented generation
- **Academic Benchmarking** - ÄÃ¡nh giÃ¡ trÃªn PAN15 Author Profiling dataset

---

## ğŸ“‹ Má»¥c lá»¥c

1. [Giá»›i thiá»‡u Big Five](#-big-five-personality-traits-ocean)
2. [Cáº¥u trÃºc Project](#-cáº¥u-trÃºc-project-chi-tiáº¿t)
3. [CÃ i Ä‘áº·t mÃ´i trÆ°á»ng](#-cÃ i-Ä‘áº·t-mÃ´i-trÆ°á»ng)
4. [Chuáº©n bá»‹ dá»¯ liá»‡u](#-chuáº©n-bá»‹-dá»¯-liá»‡u)
5. [Cháº¡y Demo nhanh](#-cháº¡y-demo-nhanh)
6. [Pipeline Ä‘áº§y Ä‘á»§](#-pipeline-Ä‘áº§y-Ä‘á»§)
7. [HÆ°á»›ng dáº«n Debug](#-hÆ°á»›ng-dáº«n-debug)
8. [Chi tiáº¿t cÃ¡c Module](#-chi-tiáº¿t-cÃ¡c-module)
9. [Chi tiáº¿t cÃ¡c Script](#-chi-tiáº¿t-cÃ¡c-script)
10. [Streamlit Application](#ï¸-streamlit-application)
11. [ÄÃ¡nh giÃ¡ & Metrics](#-Ä‘Ã¡nh-giÃ¡--metrics)
12. [Docker Deployment](#-docker-deployment)
13. [Limitations & Ethics](#ï¸-limitations--ethics)
14. [References](#-references)

---

## ğŸ¯ Big Five Personality Traits (OCEAN)

| Trait | Tiáº¿ng Viá»‡t | Description |
|-------|------------|-------------|
| **O**penness | Cá»Ÿi má»Ÿ | SÃ¡ng táº¡o, tÃ² mÃ², thÃ­ch tráº£i nghiá»‡m má»›i |
| **C**onscientiousness | Táº­n tÃ¢m | CÃ³ tá»• chá»©c, trÃ¡ch nhiá»‡m, hÆ°á»›ng má»¥c tiÃªu |
| **E**xtraversion | HÆ°á»›ng ngoáº¡i | HÃ²a Ä‘á»“ng, nÄƒng Ä‘á»™ng, cáº£m xÃºc tÃ­ch cá»±c |
| **A**greeableness | Dá»… chá»‹u | Há»£p tÃ¡c, Ä‘á»“ng cáº£m, tin tÆ°á»Ÿng |
| **S**tability | á»”n Ä‘á»‹nh | á»”n Ä‘á»‹nh cáº£m xÃºc (ngÆ°á»£c vá»›i Neuroticism) |

> **LÆ°u Ã½**: ChÃºng tÃ´i dÃ¹ng "Stable" (á»•n Ä‘á»‹nh cáº£m xÃºc) thay vÃ¬ "Neurotic" theo quy Æ°á»›c PAN15. Náº¿u cáº§n Neuroticism: `N = 1 - Stable`.

---

## ğŸ“ Cáº¥u trÃºc Project chi tiáº¿t

```
web-mining-bigfive/
â”œâ”€â”€ README.md                 # TÃ i liá»‡u nÃ y
â”œâ”€â”€ requirements.txt          # Dependencies Python
â”œâ”€â”€ .env.example              # Template biáº¿n mÃ´i trÆ°á»ng
â”œâ”€â”€ Dockerfile                # Container config
â”‚
â”œâ”€â”€ app/                      # ğŸ–¥ï¸ Streamlit Web Application
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ streamlit_app.py      # Main app (781 lines) - UI demo Ä‘áº§y Ä‘á»§
â”‚
â”œâ”€â”€ src/                      # ğŸ“¦ Source Code Modules
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py             # Cáº¥u hÃ¬nh toÃ n cá»¥c (paths, models, constants)
â”‚   â”‚
â”‚   â”œâ”€â”€ data/                 # ğŸ“Š Data Processing
â”‚   â”‚   â”œâ”€â”€ pan15_parser.py   # Parse XML data tá»« PAN15
â”‚   â”‚   â””â”€â”€ build_splits.py   # Táº¡o train/dev/test splits
â”‚   â”‚
â”‚   â”œâ”€â”€ models/               # ğŸ¤– ML Models
â”‚   â”‚   â”œâ”€â”€ tfidf_ridge.py    # TF-IDF + Ridge Baseline (~8.6KB)
â”‚   â”‚   â””â”€â”€ transformer_regressor.py  # Transformer model (~26.7KB)
â”‚   â”‚
â”‚   â”œâ”€â”€ opinion/              # ğŸ’­ Opinion Mining
â”‚   â”‚   â””â”€â”€ features.py       # Sentiment & Emotion extraction
â”‚   â”‚
â”‚   â”œâ”€â”€ ir/                   # ğŸ” Information Retrieval
â”‚   â”‚   â”œâ”€â”€ bm25.py           # BM25 indexing & search
â”‚   â”‚   â”œâ”€â”€ chroma_store.py   # ChromaDB vector store
â”‚   â”‚   â”œâ”€â”€ evidence.py       # Evidence retrieval per trait
â”‚   â”‚   â””â”€â”€ ir_eval.py        # IR evaluation (P@k, nDCG@k)
â”‚   â”‚
â”‚   â”œâ”€â”€ recsys/               # ğŸ·ï¸ Recommendation System
â”‚   â”‚   â”œâ”€â”€ hashtag_recsys.py # Main RecSys logic (~27KB)
â”‚   â”‚   â”œâ”€â”€ metrics.py        # Precision@k, Recall@k, MAP@k
â”‚   â”‚   â”œâ”€â”€ gnn_recsys.py     # LightGCN, Personality-enhanced GCN
â”‚   â”‚   â”œâ”€â”€ sasrec.py         # Sequential recommendation (SASRec)
â”‚   â”‚   â””â”€â”€ advanced_models.py# KGE, Hyperbolic GCN (~18KB)
â”‚   â”‚
â”‚   â”œâ”€â”€ rag/                  # ğŸ§  RAG Explainer
â”‚   â”‚   â”œâ”€â”€ explain.py        # LLM/Rule-based explanations
â”‚   â”‚   â””â”€â”€ prompts.py        # Prompt templates
â”‚   â”‚
â”‚   â””â”€â”€ utils/                # ğŸ”§ Utilities
â”‚       â”œâ”€â”€ io.py             # File I/O, logging setup
â”‚       â”œâ”€â”€ seed.py           # Random seed management
â”‚       â””â”€â”€ text.py           # Text preprocessing
â”‚
â”œâ”€â”€ scripts/                  # ğŸš€ Executable Scripts
â”‚   â”œâ”€â”€ download_pan15.sh     # Download dataset tá»« Zenodo
â”‚   â”œâ”€â”€ preprocess_pan15.py   # Parse & create splits
â”‚   â”œâ”€â”€ train_eval_baseline_tfidf.py  # Train TF-IDF baseline
â”‚   â”œâ”€â”€ train_eval_transformer.py     # Train Transformer model
â”‚   â”œâ”€â”€ opinion_features.py   # Extract sentiment/emotion
â”‚   â”œâ”€â”€ build_ir_index.py     # Build BM25 index
â”‚   â”œâ”€â”€ retrieve_evidence.py  # Retrieve evidence tweets
â”‚   â”œâ”€â”€ build_chroma_db.py    # Build ChromaDB
â”‚   â”œâ”€â”€ recsys_eval.py        # Evaluate RecSys (963 lines!)
â”‚   â”œâ”€â”€ build_recsys_dataset.py # Build RecSys evaluation dataset
â”‚   â”œâ”€â”€ ir_label_tool.py      # Manual IR labeling tool
â”‚   â”œâ”€â”€ run_full_experiment.py# Run full pipeline
â”‚   â”œâ”€â”€ consolidate_new.py    # Consolidate results
â”‚   â”œâ”€â”€ plot_from_logs.py     # Plot training curves
â”‚   â””â”€â”€ regenerate_plots.py   # Regenerate visualizations
â”‚
â”œâ”€â”€ data/                     # ğŸ“ Data Directory
â”‚   â”œâ”€â”€ raw/                  # Downloaded raw PAN15 data
â”‚   â”‚   â”œâ”€â”€ pan15_train/
â”‚   â”‚   â”œâ”€â”€ pan15_test/
â”‚   â”‚   â””â”€â”€ pan15_train_en/
â”‚   â”œâ”€â”€ processed/            # Processed parquet/pkl files
â”‚   â”‚   â”œâ”€â”€ pan15_en.parquet
â”‚   â”‚   â”œâ”€â”€ pan15_es.parquet
â”‚   â”‚   â”œâ”€â”€ chroma_db/
â”‚   â”‚   â”œâ”€â”€ ir_bm25.pkl
â”‚   â”‚   â””â”€â”€ evidence_topk.parquet
â”‚   â””â”€â”€ splits/               # User ID splits
â”‚       â”œâ”€â”€ en/
â”‚       â”œâ”€â”€ es/
â”‚       â”œâ”€â”€ it/
â”‚       â””â”€â”€ nl/
â”‚
â”œâ”€â”€ models/                   # ğŸ’¾ Saved Model Checkpoints
â”‚   â”œâ”€â”€ baseline_en.joblib
â”‚   â””â”€â”€ transformer_en.pt
â”‚
â”œâ”€â”€ results/                  # ğŸ“ˆ Evaluation Results
â”‚   â”œâ”€â”€ metrics_baseline_en.csv
â”‚   â””â”€â”€ metrics_transformer_en.csv
â”‚
â”œâ”€â”€ tests/                    # ğŸ§ª Test Files
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ report.md                 # ğŸ“„ BÃ¡o cÃ¡o RecSys (tiáº¿ng Viá»‡t)
â””â”€â”€ report_recsys_summary.md  # ğŸ“„ TÃ³m táº¯t RecSys
```

---

## ğŸ› ï¸ CÃ i Ä‘áº·t mÃ´i trÆ°á»ng

### YÃªu cáº§u há»‡ thá»‘ng

- **Python**: 3.10+ (khuyáº¿n nghá»‹ 3.11)
- **RAM**: Tá»‘i thiá»ƒu 8GB, khuyáº¿n nghá»‹ 16GB
- **GPU**: Optional, cáº§n cho Transformer training (CUDA 11.8+)
- **Disk**: ~5GB cho data + models

### BÆ°á»›c 1: Clone repository

```bash
git clone <repository-url>
cd web-mining-bigfive
```

### BÆ°á»›c 2: Táº¡o Virtual Environment

```bash
# Sá»­ dá»¥ng Python 3.11
python3.11 -m venv venv
source venv/bin/activate  # Linux/Mac
# hoáº·c: venv\Scripts\activate  # Windows

# Verify Python version
python --version  # Pháº£i lÃ  3.10+
```

### BÆ°á»›c 3: CÃ i Ä‘áº·t Dependencies

```bash
pip install --upgrade pip
pip install -r requirements.txt
```

**Danh sÃ¡ch dependencies chÃ­nh:**

| Package | Version | Má»¥c Ä‘Ã­ch |
|---------|---------|----------|
| `pandas` | â‰¥2.0.0 | Data manipulation |
| `numpy` | â‰¥1.24.0 | Numerical computing |
| `scikit-learn` | â‰¥1.3.0 | TF-IDF, Ridge, metrics |
| `torch` | â‰¥2.0.0 | Deep learning |
| `transformers` | â‰¥4.35.0 | HuggingFace models |
| `sentence-transformers` | â‰¥2.2.0 | Sentence embeddings |
| `rank-bm25` | â‰¥0.2.2 | BM25 indexing |
| `chromadb` | â‰¥0.4.0 | Vector database |
| `streamlit` | â‰¥1.28.0 | Web UI |
| `matplotlib` | â‰¥3.7.0 | Plotting |
| `plotly` | â‰¥5.18.0 | Interactive charts |
| `openai` | â‰¥1.0.0 | RAG explanations |

### BÆ°á»›c 4: Cáº¥u hÃ¬nh Environment Variables

```bash
cp .env.example .env
```

Chá»‰nh sá»­a `.env`:

```env
OPENAI_API_KEY=your_openai_api_key_here  # Optional - cho RAG
OPENAI_MODEL=gpt-3.5-turbo               # Optional
```

> **LÆ°u Ã½**: OpenAI API key chá»‰ cáº§n cho tÃ­nh nÄƒng RAG explanations. Demo váº«n cháº¡y Ä‘Æ°á»£c mÃ  khÃ´ng cáº§n.

### Troubleshooting cÃ i Ä‘áº·t

<details>
<summary>âŒ <b>Lá»—i: torch khÃ´ng cÃ i Ä‘Æ°á»£c trÃªn GPU</b></summary>

```bash
# CÃ i PyTorch vá»›i CUDA 11.8
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# Kiá»ƒm tra CUDA
python -c "import torch; print(torch.cuda.is_available())"
```
</details>

<details>
<summary>âŒ <b>Lá»—i: chromadb build fails</b></summary>

```bash
# CÃ i build tools
sudo apt-get install build-essential python3-dev

# Hoáº·c dÃ¹ng pre-built wheel
pip install chromadb --prefer-binary
```
</details>

<details>
<summary>âŒ <b>Lá»—i: ModuleNotFoundError</b></summary>

```bash
# Äáº£m báº£o PYTHONPATH Ä‘Ãºng
export PYTHONPATH="${PWD}:${PYTHONPATH}"

# Hoáº·c cháº¡y tá»« root directory
cd /path/to/web-mining-bigfive
python scripts/your_script.py
```
</details>

---

## ğŸ“¥ Chuáº©n bá»‹ dá»¯ liá»‡u

### Download PAN15 Dataset

```bash
# Download tá»« Zenodo (khoáº£ng 500MB)
bash scripts/download_pan15.sh
```

Script sáº½:
1. Download file zip tá»« Zenodo
2. Extract vÃ o `data/raw/`
3. Táº¡o cÃ¡c thÆ° má»¥c cáº§n thiáº¿t

### Preprocess Data

```bash
# Parse XML vÃ  táº¡o train/dev/test splits
python scripts/preprocess_pan15.py
```

**Output:**
- `data/processed/pan15_{lang}.parquet` - Dá»¯ liá»‡u Ä‘Ã£ xá»­ lÃ½
- `data/splits/{lang}/train.txt` - User IDs cho training
- `data/splits/{lang}/dev.txt` - User IDs cho validation
- `data/splits/{lang}/test.txt` - User IDs cho testing

**NgÃ´n ngá»¯ há»— trá»£:** `en` (English), `es` (Spanish), `it` (Italian), `nl` (Dutch)

---

## ğŸš€ Cháº¡y Demo nhanh

### Option 1: Demo vá»›i dá»¯ liá»‡u máº«u (Nhanh nháº¥t)

```bash
# 1. Preprocess (náº¿u chÆ°a cÃ³ data)
python scripts/preprocess_pan15.py

# 2. Train baseline model vá»›i sample nhá»
python scripts/train_eval_baseline_tfidf.py --sample_size 200

# 3. Build BM25 index cho evidence
python scripts/build_ir_index.py --sample_size 200

# 4. Cháº¡y Streamlit app
streamlit run app/streamlit_app.py
```

Má»Ÿ browser táº¡i: **http://localhost:8501**

### Option 2: Demo vá»›i model Ä‘áº§y Ä‘á»§

```bash
# Train full baseline
python scripts/train_eval_baseline_tfidf.py --lang en

# Cháº¡y app
streamlit run app/streamlit_app.py
```

---

## ğŸ”„ Pipeline Ä‘áº§y Ä‘á»§

Thá»±c hiá»‡n tuáº§n tá»± cÃ¡c bÆ°á»›c sau:

### 1. Baseline Training

```bash
# TF-IDF + Ridge Regression
python scripts/train_eval_baseline_tfidf.py --lang en --alpha 1.0
```

### 2. Opinion Mining Features

```bash
# Extract sentiment/emotion features
python scripts/opinion_features.py

# Train vá»›i opinion features
python scripts/train_eval_baseline_tfidf.py --with_opinion --lang en
```

### 3. Information Retrieval

```bash
# Build BM25 index
python scripts/build_ir_index.py

# Retrieve evidence tweets per trait
python scripts/retrieve_evidence.py
```

### 4. Vector Database (cho RAG)

```bash
# Build ChromaDB vá»›i sentence embeddings
python scripts/build_chroma_db.py
```

### 5. Recommendation System Evaluation

```bash
# Build RecSys dataset
python scripts/build_recsys_dataset.py

# Evaluate RecSys (comprehensive)
python scripts/recsys_eval.py --k 10
```

### 6. Transformer Training (GPU required)

```bash
# Train vá»›i Twitter-RoBERTa (English)
python scripts/train_eval_transformer.py \
    --lang en \
    --epochs 50 \
    --batch_size 8 \
    --lr 2e-5 \
    --early_stopping 10

# Train vá»›i XLM-RoBERTa (Multilingual)
python scripts/train_eval_transformer.py \
    --lang es \
    --model_name xlm-roberta-base \
    --epochs 30
```

### 7. Launch Application

```bash
streamlit run app/streamlit_app.py
```

---

## ğŸ› HÆ°á»›ng dáº«n Debug

### Debug vá»›i logging

Má»i script Ä‘á»u cÃ³ logging tá»± Ä‘á»™ng:

```bash
# Xem logs chi tiáº¿t
python scripts/train_eval_baseline_tfidf.py --lang en 2>&1 | tee training.log
```

### Debug step-by-step trong Python

```python
import sys
sys.path.insert(0, '/path/to/web-mining-bigfive')

# Load config
from src.config import *
print(f"Data dir: {DATA_DIR}")
print(f"Models dir: {MODELS_DIR}")

# Load data
from src.utils.io import load_parquet, load_splits
df = load_parquet(PROCESSED_DIR / "pan15_en.parquet")
print(f"Loaded {len(df)} users")

# Load model
from src.models.tfidf_ridge import TfidfRidgeModel
model = TfidfRidgeModel.load(MODELS_DIR / "baseline_en.joblib")

# Test prediction
sample_text = "I love trying new things and meeting new people!"
pred = model.predict([sample_text])
print(f"Predictions: {pred}")
```

### Debug Streamlit App

```bash
# Cháº¡y vá»›i hot-reload vÃ  logs
streamlit run app/streamlit_app.py --logger.level=debug

# Hoáº·c vá»›i Python debugger
python -m pdb -c continue app/streamlit_app.py
```

### Debug RecSys

```bash
# Cháº¡y vá»›i verbose output
python scripts/recsys_eval.py --k 10 2>&1 | tee recsys_debug.log

# Kiá»ƒm tra tá»«ng method
python -c "
from src.recsys.hashtag_recsys import HashtagRecommender
from src.config import PROCESSED_DIR

# Load recommender
rec = HashtagRecommender()
rec.fit(PROCESSED_DIR / 'pan15_en.parquet')

# Test
recs = rec.recommend(user_tags=['happy', 'travel'], k=5)
print(recs)
"
```

### Kiá»ƒm tra GPU/CUDA

```python
import torch
print(f"CUDA available: {torch.cuda.is_available()}")
print(f"CUDA version: {torch.version.cuda}")
print(f"Device count: {torch.cuda.device_count()}")
if torch.cuda.is_available():
    print(f"Current device: {torch.cuda.current_device()}")
    print(f"Device name: {torch.cuda.get_device_name(0)}")
```

### Common Issues & Solutions

<details>
<summary>âŒ <b>"Model file not found"</b></summary>

Cháº¡y training trÆ°á»›c:
```bash
python scripts/train_eval_baseline_tfidf.py --lang en
```
</details>

<details>
<summary>âŒ <b>"Data file not found"</b></summary>

Cháº¡y preprocessing:
```bash
bash scripts/download_pan15.sh
python scripts/preprocess_pan15.py
```
</details>

<details>
<summary>âŒ <b>"CUDA out of memory"</b></summary>

Giáº£m batch size:
```bash
python scripts/train_eval_transformer.py --batch_size 4
```
</details>

<details>
<summary>âŒ <b>"Streamlit connection refused"</b></summary>

```bash
# Kiá»ƒm tra port
lsof -i :8501

# Äá»•i port náº¿u cáº§n
streamlit run app/streamlit_app.py --server.port 8502
```
</details>

---

## ğŸ“¦ Chi tiáº¿t cÃ¡c Module

### `src/config.py` - Configuration

```python
# Paths
PROJECT_ROOT = Path(__file__).parent.parent
DATA_DIR = PROJECT_ROOT / "data"
PROCESSED_DIR = DATA_DIR / "processed"
MODELS_DIR = PROJECT_ROOT / "models"

# Constants
TRAIT_NAMES = ["open", "conscientious", "extroverted", "agreeable", "stable"]
SEED = 42

# Models
SENTIMENT_MODEL = "cardiffnlp/twitter-roberta-base-sentiment-latest"
EMOTION_MODEL = "cardiffnlp/twitter-roberta-base-emotion"
ENCODER_MODEL = "cardiffnlp/twitter-roberta-base"
MULTILINGUAL_ENCODER_MODEL = "xlm-roberta-base"
EMBEDDING_MODEL = "sentence-transformers/all-MiniLM-L6-v2"
```

---

### `src/models/` - ML Models

#### `tfidf_ridge.py` - Baseline Model

```python
from src.models.tfidf_ridge import TfidfRidgeModel

# Train
model = TfidfRidgeModel(alpha=1.0)
model.fit(train_texts, train_labels)

# Predict
predictions = model.predict(test_texts)  # shape: (n_samples, 5)

# Evaluate
metrics = model.evaluate(test_texts, test_labels)
# Returns: {'rmse_avg': 0.15, 'rmse_open': 0.14, ...}

# Save/Load
model.save("model.joblib")
model = TfidfRidgeModel.load("model.joblib")
```

#### `transformer_regressor.py` - Transformer Model

```python
from src.models.transformer_regressor import TransformerTrainer

trainer = TransformerTrainer(
    model_name="cardiffnlp/twitter-roberta-base",
    learning_rate=2e-5,
    batch_size=8,
    max_length=512
)

trainer.fit(
    train_texts, train_targets,
    val_texts, val_targets,
    epochs=50,
    early_stopping_patience=10
)

predictions = trainer.predict(test_texts)
```

---

### `src/ir/` - Information Retrieval

#### `bm25.py` - BM25 Index

```python
from src.ir.bm25 import BM25Index

# Build index
index = BM25Index()
index.fit(documents)  # List of strings

# Search
results = index.search("creative ideas", top_k=5)
# Returns: [(doc_idx, score), ...]

# Save/Load
index.save("bm25.pkl")
index = BM25Index.load("bm25.pkl")
```

#### `chroma_store.py` - Vector Database

```python
from src.ir.chroma_store import ChromaStore

store = ChromaStore(persist_dir="data/processed/chroma_db")
store.add_users(user_ids, embeddings, metadata)

# Find similar users
similar = store.query(query_embedding, k=5)
```

#### `evidence.py` - Evidence Retrieval

```python
from src.ir.evidence import EvidenceRetriever

retriever = EvidenceRetriever(bm25_index)
evidence = retriever.get_evidence_for_traits(k=5)
# Returns: {trait: [(tweet, score), ...]}
```

---

### `src/recsys/` - Recommendation System

#### `hashtag_recsys.py` - Main RecSys

```python
from src.recsys.hashtag_recsys import HashtagRecommender

rec = HashtagRecommender()
rec.fit(data_path)

# Methods available:
# - popularity(): Global popularity baseline
# - content_based(user_profile): Embedding similarity
# - personality_aware(user_traits): Trait matching
# - hybrid(user_profile, user_traits): Combined approach

recs = rec.recommend(
    user_tags=['happy', 'travel'],
    user_traits=[0.7, 0.6, 0.8, 0.5, 0.6],
    method='hybrid',
    k=10
)
```

#### `gnn_recsys.py` - Graph Neural Networks

```python
from src.recsys.gnn_recsys import PersonalityLightGCN

model = PersonalityLightGCN(
    n_users=1000,
    n_items=500,
    embedding_dim=64,
    n_layers=3
)
```

#### `metrics.py` - Evaluation Metrics

```python
from src.recsys.metrics import precision_at_k, recall_at_k, map_at_k

p = precision_at_k(recommendations, ground_truth, k=10)
r = recall_at_k(recommendations, ground_truth, k=10)
m = map_at_k(recommendations, ground_truth, k=10)
```

---

### `src/opinion/` - Opinion Mining

```python
from src.opinion.features import OpinionFeatureExtractor

extractor = OpinionFeatureExtractor()
features = extractor.extract(texts)
# Returns: np.array with sentiment/emotion features
# - Sentiment probabilities (neg, neu, pos)
# - Emotion probabilities (anger, joy, ...)
# - Entropy, positive/negative rates
```

---

### `src/rag/` - RAG Explainer

```python
from src.rag.explain import PersonalityExplainer

explainer = PersonalityExplainer(api_key="...")

# With OpenAI
explanation = explainer.explain(
    traits={'open': 0.8, 'extroverted': 0.7, ...},
    evidence={'open': ["I love art!", ...], ...}
)

# Fallback (rule-based, no API key)
explanation = explainer.explain_rule_based(traits)
```

---

## ğŸ“œ Chi tiáº¿t cÃ¡c Script

### `scripts/train_eval_baseline_tfidf.py`

Train vÃ  evaluate TF-IDF + Ridge baseline.

```bash
python scripts/train_eval_baseline_tfidf.py \
    --lang en                    # Language: en, es, it, nl
    --with_opinion               # Include sentiment features
    --sample_size 200            # Limit samples (for testing)
    --alpha 1.0                  # Ridge alpha parameter
    --seed 42                    # Random seed
    --results_dir ./results      # Output directory
```

**Output:**
- `models/baseline_{lang}.joblib` - Trained model
- `results/metrics_baseline_{lang}.csv` - Evaluation metrics

---

### `scripts/train_eval_transformer.py`

Train Transformer regressor.

```bash
python scripts/train_eval_transformer.py \
    --lang en                    # Language code
    --model_name cardiffnlp/twitter-roberta-base  # HF model
    --epochs 50                  # Max epochs
    --batch_size 8               # Batch size (reduce if OOM)
    --lr 2e-5                    # Learning rate
    --max_length 512             # Max sequence length
    --early_stopping 10          # Patience for early stopping
    --warmup_epochs 2            # Warmup epochs
    --no_cosine                  # Disable cosine scheduler
    --sample_size 100            # Limit samples
    --seed 42                    # Random seed
    --results_dir ./results      # Output directory
```

**Output:**
- `models/transformer_{lang}.pt` - Trained model
- `results/metrics_transformer_{lang}.csv` - Metrics

---

### `scripts/recsys_eval.py`

Comprehensive RecSys evaluation (963 lines).

```bash
python scripts/recsys_eval.py \
    --k 10                       # Top-k recommendations
    --lang en                    # Language
```

**Evaluates:**
- Popularity baseline
- Content-based filtering
- Personality-aware filtering
- Hybrid methods
- LightGCN
- SASRec (Sequential)
- KGE, Hyperbolic GCN

---

### `scripts/build_recsys_dataset.py`

Build dataset for RecSys evaluation.

```bash
python scripts/build_recsys_dataset.py
```

**Creates:**
- `data/processed/recsys_train.parquet`
- `data/processed/recsys_test.parquet`

---

### `scripts/opinion_features.py`

Extract sentiment/emotion features.

```bash
python scripts/opinion_features.py
```

**Creates:**
- `data/processed/opinion_features.parquet`

---

### `scripts/build_ir_index.py`

Build BM25 index for evidence retrieval.

```bash
python scripts/build_ir_index.py \
    --sample_size 1000           # Optional: limit documents
```

**Creates:**
- `data/processed/ir_bm25.pkl`

---

### `scripts/retrieve_evidence.py`

Retrieve evidence tweets for each trait.

```bash
python scripts/retrieve_evidence.py
```

**Creates:**
- `data/processed/evidence_topk.parquet`

---

### `scripts/build_chroma_db.py`

Build ChromaDB vector store.

```bash
python scripts/build_chroma_db.py
```

**Creates:**
- `data/processed/chroma_db/`

---

## ğŸ–¥ï¸ Streamlit Application

### Features

1. **ğŸ“ Text Input**: Paste text hoáº·c upload file
2. **ğŸ”® Personality Prediction**: Dá»± Ä‘oÃ¡n 5 traits vá»›i confidence
3. **ğŸ“Š Visualization**: Radar chart tÆ°Æ¡ng tÃ¡c (Plotly)
4. **ğŸ“‘ Evidence**: Tweets liÃªn quan Ä‘áº¿n tá»«ng trait
5. **ğŸ’¡ AI Explanation**: Giáº£i thÃ­ch dá»±a trÃªn RAG/Rules
6. **ğŸ·ï¸ Hashtag Recommendations**: Gá»£i Ã½ hashtag cÃ¡ nhÃ¢n hÃ³a

### Configuration trong App

```python
# app/streamlit_app.py

# MÃ u sáº¯c cho tá»«ng trait
TRAIT_COLORS = {
    "open": "#f59e0b",        # Amber
    "conscientious": "#10b981",# Emerald
    "extroverted": "#f43f5e", # Rose
    "agreeable": "#3b82f6",   # Blue
    "stable": "#8b5cf6",      # Purple
}

# Icons
TRAIT_ICONS = {
    "open": "ğŸ¨",
    "conscientious": "ğŸ“‹",
    "extroverted": "ğŸ‰",
    "agreeable": "ğŸ¤",
    "stable": "ğŸ§˜",
}
```

### Customization

Chá»‰nh sá»­a `app/streamlit_app.py`:

```python
# Thay Ä‘á»•i page config
st.set_page_config(
    page_title="Your Title",
    page_icon="ğŸ§ ",
    layout="wide",
)

# Thay Ä‘á»•i sidebar
with st.sidebar:
    st.title("Your Sidebar")
```

---

## ğŸ“Š ÄÃ¡nh giÃ¡ & Metrics

### Personality Prediction

| Metric | Description | Formula |
|--------|-------------|---------|
| **RMSE** | Root Mean Squared Error | $\sqrt{\frac{1}{n}\sum(y - \hat{y})^2}$ |
| **MAE** | Mean Absolute Error | $\frac{1}{n}\sum|y - \hat{y}|$ |
| **Avg RMSE** | Average across 5 traits | $\frac{1}{5}\sum RMSE_i$ |

### Information Retrieval

| Metric | Description |
|--------|-------------|
| **P@k** | Precision at k |
| **nDCG@k** | Normalized DCG |

### Recommendation System

| Metric | Description |
|--------|-------------|
| **Precision@k** | Relevant items in top-k / k |
| **Recall@k** | Relevant items in top-k / total relevant |
| **MAP@k** | Mean Average Precision at k |

### Sample Results

| Model | RMSE (avg) | Notes |
|-------|------------|-------|
| TF-IDF + Ridge | ~0.15 | Fast, baseline |
| TF-IDF + Opinion | ~0.14 | +Sentiment features |
| Twitter-RoBERTa | ~0.12 | Transformer, GPU |

---

## ğŸ³ Docker Deployment

### Build Image

```bash
docker build -t bigfive-analyzer .
```

### Run Container

```bash
docker run -p 8501:8501 \
    -v $(pwd)/data:/app/data \
    -v $(pwd)/models:/app/models \
    -e OPENAI_API_KEY=your_key \
    bigfive-analyzer
```

### Docker Compose (Optional)

```yaml
version: '3.8'
services:
  app:
    build: .
    ports:
      - "8501:8501"
    volumes:
      - ./data:/app/data
      - ./models:/app/models
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
```

---

## âš ï¸ Limitations & Ethics

### Limitations

1. **Dataset Bias**: PAN15 data tá»« 2015, cÃ³ thá»ƒ khÃ´ng pháº£n Ã¡nh ngÃ´n ngá»¯ hiá»‡n Ä‘áº¡i
2. **English Focus**: Model chÃ­nh train trÃªn English
3. **Aggregation**: Dá»± Ä‘oÃ¡n user-level tá»« limited posts
4. **Temporal**: Personality cÃ³ thá»ƒ thay Ä‘á»•i theo thá»i gian

### Ethical Considerations

1. **Privacy**: KHÃ”NG sá»­ dá»¥ng cho profiling trÃ¡i phÃ©p
2. **Consent**: Chá»‰ analyze content cÃ³ consent
3. **Not Diagnostic**: ÄÃ¢y lÃ  ML predictions, KHÃ”NG pháº£i clinical assessment
4. **Bias Awareness**: Models cÃ³ thá»ƒ perpetuate training data biases
5. **Transparency**: LuÃ´n disclose khi apply personality analysis

### Data Usage

- Training chá»‰ dÃ¹ng public benchmark datasets (PAN15)
- KhÃ´ng real-time X/Twitter API crawling
- Demo input lÃ  user-provided only

---

## ğŸ“š References

1. **PAN15 Author Profiling Task**
   - Rangel, F., et al. "Overview of the 3rd Author Profiling Task at PAN 2015"
   - [PAN @ CLEF 2015](https://pan.webis.de/clef15/pan15-web/author-profiling.html)

2. **CardiffNLP Twitter Models**
   - https://github.com/cardiffnlp/twitter-models
   - Twitter-RoBERTa for sentiment, emotion, etc.

3. **Big Five Personality Model**
   - Costa, P. T., & McCrae, R. R. (1992). "Revised NEO Personality Inventory"

4. **Personality-Aware Recommendation**
   - Tkalcic, M., & Chen, L. (2015). "Personality and recommender systems"

5. **Hybrid Recommender Systems**
   - Burke, R. (2002). "Hybrid recommender systems: Survey and experiments"

---

## ğŸ“„ License

MIT License - See LICENSE file for details.

---

## ğŸ‘¥ Contributors

**Course**: Web Mining  
**Capstone Project**: Detection of Big Five Personality Traits from Social Media

---

## ğŸ†˜ Support

Náº¿u gáº·p váº¥n Ä‘á»:

1. Check [Troubleshooting](#troubleshooting-cÃ i-Ä‘áº·t) section
2. Review logs trong `*.log` files
3. Má»Ÿ issue trÃªn GitHub vá»›i:
   - Error message Ä‘áº§y Ä‘á»§
   - Python version (`python --version`)
   - OS information
   - Steps to reproduce

---

> **Last Updated**: January 2026
